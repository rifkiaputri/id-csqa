{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV Rephrased Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, bool_params=[]):\n",
    "    # Initialize an empty list to store the data\n",
    "    data_list = []\n",
    "\n",
    "    # Open the CSV file for reading\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Append the row (as a dictionary) to the data_list\n",
    "            row[\"choices\"] = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "            for param in bool_params:\n",
    "                if row[param] == \"True\" or row[param] == \"TRUE\":\n",
    "                    row[param] = True\n",
    "                elif row[param] == \"False\" or row[param] == \"FALSE\":\n",
    "                    row[param] = False\n",
    "                else:\n",
    "                    raise TypeError(f\"{param} data cannot be recognized\")\n",
    "\n",
    "            data_list.append(row)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def load_all_rephrase_data(split, prompt_ver, dir_path, file_name, bool_params=[]):\n",
    "    data = {}\n",
    "    \n",
    "    for s in split:\n",
    "        file_path = f\"{dir_path}/{s}/{prompt_ver}_{s}_{file_name}\"\n",
    "        data[s] = load_csv_data(file_path, bool_params)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "split = [\"validation\", \"test\", \"train\"]\n",
    "name_params = [\"concept\", \"name\", \"option\", \"verdict\"]\n",
    "all_params = [\"concept\", \"name\", \"option\", \"low_verdict\", \"high_verdict\"]\n",
    "\n",
    "evaluated_data = {\n",
    "    \"name\": [\n",
    "        load_all_rephrase_data(split, \"v3\", \"eval\", \"name.csv\", bool_params=name_params),\n",
    "    ],\n",
    "    \"both\": [\n",
    "        load_all_rephrase_data(split, \"v3\", \"eval\", \"both.csv\", bool_params=all_params),\n",
    "    ],\n",
    "    \"concept\": [\n",
    "        load_all_rephrase_data(split, \"v3\", \"eval\", \"concept.csv\", bool_params=all_params),\n",
    "    ],\n",
    "    \"option\": [\n",
    "        load_all_rephrase_data(split, \"v3\", \"eval\", \"option.csv\", bool_params=all_params),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Data Split: validation\n",
      "\n",
      "Evaluation Attributes: name\n",
      "Number of Sampled Data: 51\n",
      "v3 Prompt Results:\n",
      "    - True: 48\n",
      "    - False: 3\n",
      "    - Accuracy: 0.9411764705882353\n",
      "\n",
      "Evaluation Attributes: both\n",
      "Number of Sampled Data: 10\n",
      "v3 Prompt Results:\n",
      "    - True: 5\n",
      "    - False: 5\n",
      "    - Accuracy: 0.5\n",
      "\n",
      "Evaluation Attributes: concept\n",
      "Number of Sampled Data: 23\n",
      "v3 Prompt Results:\n",
      "    - True: 19\n",
      "    - False: 4\n",
      "    - Accuracy: 0.8260869565217391\n",
      "\n",
      "Evaluation Attributes: option\n",
      "Number of Sampled Data: 52\n",
      "v3 Prompt Results:\n",
      "    - True: 42\n",
      "    - False: 10\n",
      "    - Accuracy: 0.8076923076923077\n",
      "\n",
      "Rephrase All Evaluation Conclusion\n",
      "Number of Sampled Data: 85\n",
      "v3 Prompt Results:\n",
      "    - True: 66\n",
      "    - False: 19\n",
      "    - Accuracy: 0.7764705882352941\n",
      "====================================\n",
      "Data Split: test\n",
      "\n",
      "Evaluation Attributes: name\n",
      "Number of Sampled Data: 46\n",
      "v3 Prompt Results:\n",
      "    - True: 44\n",
      "    - False: 2\n",
      "    - Accuracy: 0.9565217391304348\n",
      "\n",
      "Evaluation Attributes: both\n",
      "Number of Sampled Data: 8\n",
      "v3 Prompt Results:\n",
      "    - True: 6\n",
      "    - False: 2\n",
      "    - Accuracy: 0.75\n",
      "\n",
      "Evaluation Attributes: concept\n",
      "Number of Sampled Data: 17\n",
      "v3 Prompt Results:\n",
      "    - True: 14\n",
      "    - False: 3\n",
      "    - Accuracy: 0.8235294117647058\n",
      "\n",
      "Evaluation Attributes: option\n",
      "Number of Sampled Data: 46\n",
      "v3 Prompt Results:\n",
      "    - True: 36\n",
      "    - False: 10\n",
      "    - Accuracy: 0.782608695652174\n",
      "\n",
      "Rephrase All Evaluation Conclusion\n",
      "Number of Sampled Data: 71\n",
      "v3 Prompt Results:\n",
      "    - True: 56\n",
      "    - False: 15\n",
      "    - Accuracy: 0.7887323943661971\n",
      "====================================\n",
      "Data Split: train\n",
      "\n",
      "Evaluation Attributes: name\n",
      "Number of Sampled Data: 90\n",
      "v3 Prompt Results:\n",
      "    - True: 87\n",
      "    - False: 3\n",
      "    - Accuracy: 0.9666666666666667\n",
      "\n",
      "Evaluation Attributes: both\n",
      "Number of Sampled Data: 16\n",
      "v3 Prompt Results:\n",
      "    - True: 10\n",
      "    - False: 6\n",
      "    - Accuracy: 0.625\n",
      "\n",
      "Evaluation Attributes: concept\n",
      "Number of Sampled Data: 27\n",
      "v3 Prompt Results:\n",
      "    - True: 22\n",
      "    - False: 5\n",
      "    - Accuracy: 0.8148148148148148\n",
      "\n",
      "Evaluation Attributes: option\n",
      "Number of Sampled Data: 81\n",
      "v3 Prompt Results:\n",
      "    - True: 60\n",
      "    - False: 21\n",
      "    - Accuracy: 0.7407407407407407\n",
      "\n",
      "Rephrase All Evaluation Conclusion\n",
      "Number of Sampled Data: 124\n",
      "v3 Prompt Results:\n",
      "    - True: 92\n",
      "    - False: 32\n",
      "    - Accuracy: 0.7419354838709677\n"
     ]
    }
   ],
   "source": [
    "for s in split:\n",
    "    print(\"====================================\")\n",
    "    print(f\"Data Split: {s}\")\n",
    "\n",
    "    all_data = [0]\n",
    "    all_true = [0]\n",
    "\n",
    "    for eval_type, eval_data in evaluated_data.items():\n",
    "        if eval_type == \"name\":\n",
    "            eval_attr = \"verdict\"\n",
    "        else:\n",
    "            eval_attr = \"high_verdict\"\n",
    "\n",
    "        # assert len(eval_data[0][s]) == len(eval_data[1][s])\n",
    "\n",
    "        v1_true = sum(1 for d in eval_data[0][s] if d[eval_attr])\n",
    "        # v2_true = sum(1 for d in eval_data[1][s] if d[eval_attr])\n",
    "\n",
    "        if eval_type != \"name\":\n",
    "            all_data[0] += len(eval_data[0][s])\n",
    "            # all_data[1] += len(eval_data[1][s])\n",
    "            all_true[0] += v1_true\n",
    "            # all_true[1] += v2_true\n",
    "\n",
    "        print(f\"\"\"\n",
    "Evaluation Attributes: {eval_type}\n",
    "Number of Sampled Data: {len(eval_data[0][s])}\n",
    "v3 Prompt Results:\n",
    "    - True: {v1_true}\n",
    "    - False: {len(eval_data[0][s]) - v1_true}\n",
    "    - Accuracy: {v1_true / len(eval_data[0][s])}\"\"\")\n",
    "\n",
    "    # assert all_data[0] == all_data[1]\n",
    "    print(f\"\"\"\n",
    "Rephrase All Evaluation Conclusion\n",
    "Number of Sampled Data: {all_data[0]}\n",
    "v3 Prompt Results:\n",
    "    - True: {all_true[0]}\n",
    "    - False: {all_data[0] - all_true[0]}\n",
    "    - Accuracy: {all_true[0] / all_data[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_csv_data(file_path):\n",
    "    data_list = []\n",
    "\n",
    "    with open(file_path, newline=\"\") as csvfile:\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        for row in csv_reader:\n",
    "            row[\"choices\"] = ast.literal_eval(row[\"choices\"])\n",
    "            data_list.append(row)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def save_data(samples, file_path):\n",
    "    header = samples[0].keys()\n",
    "\n",
    "    with open(file_path, \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for row in samples:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f'CSV file \"{file_path}\" has been created with the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining file v3_test_name.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:00, 262858.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/test/eval_v3_test_name.csv\" has been created with the data.\n",
      "Combining file v3_test_both.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 113359.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/test/eval_v3_test_both.csv\" has been created with the data.\n",
      "Combining file v3_test_concept.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 193758.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/test/eval_v3_test_concept.csv\" has been created with the data.\n",
      "Combining file v3_test_option.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [00:00, 189154.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/test/eval_v3_test_option.csv\" has been created with the data.\n",
      "Combining file v3_validation_name.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:00, 393216.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/validation/eval_v3_validation_name.csv\" has been created with the data.\n",
      "Combining file v3_validation_both.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 123725.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/validation/eval_v3_validation_both.csv\" has been created with the data.\n",
      "Combining file v3_validation_concept.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, 194493.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/validation/eval_v3_validation_concept.csv\" has been created with the data.\n",
      "Combining file v3_validation_option.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:00, 302501.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/validation/eval_v3_validation_option.csv\" has been created with the data.\n",
      "Combining file v3_train_name.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:00, 308656.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/train/eval_v3_train_name.csv\" has been created with the data.\n",
      "Combining file v3_train_both.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 100312.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/train/eval_v3_train_both.csv\" has been created with the data.\n",
      "Combining file v3_train_concept.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [00:00, 208173.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/train/eval_v3_train_concept.csv\" has been created with the data.\n",
      "Combining file v3_train_option.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 362581.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"./eval/train/eval_v3_train_option.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for folder in glob(\"./eval/*/\"):\n",
    "    for file in glob(f\"{folder}*.csv\"):\n",
    "        if \"v3\" in file:\n",
    "            fname = file.split(\"/\")[-1]\n",
    "            if \"eval\" not in fname:\n",
    "                print(f\"Combining file {fname}\")\n",
    "\n",
    "                combined_data = []\n",
    "                file_data = load_csv_data(file)\n",
    "                sm4t_med_data = load_csv_data(f\"{folder}sm4t_med/translated_sm4t_med_{fname}\")\n",
    "                sm4t_large_data = load_csv_data(f\"{folder}sm4t_large/translated_sm4t_large_{fname}\")\n",
    "                gpt_data = load_csv_data(f\"{folder}chatgpt-instruct/translated_chatgpt_instruct_{fname}\")\n",
    "\n",
    "                for idx, data in tqdm(enumerate(file_data)):\n",
    "                    data[\"translator\"] = \"raw\"\n",
    "                    sm4t_med_data[idx][\"translator\"] = \"sm4t_med\"\n",
    "                    sm4t_large_data[idx][\"translator\"] = \"sm4t_large\"\n",
    "                    gpt_data[idx][\"translator\"] = \"gpt_instruct\"\n",
    "\n",
    "                    combined_data.append(data)\n",
    "                    combined_data.append(sm4t_med_data[idx])\n",
    "                    combined_data.append(sm4t_large_data[idx])\n",
    "                    combined_data.append(gpt_data[idx])\n",
    "                \n",
    "                save_data(combined_data, f\"{folder}eval_{fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
