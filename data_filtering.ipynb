{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, bool_params):\n",
    "    # Initialize an empty list to store the data\n",
    "    data_list = []\n",
    "\n",
    "    # Open the CSV file for reading\n",
    "    with open(file_path, newline='', encoding=\"utf-8\") as csvfile:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Append the row (as a dictionary) to the data_list\n",
    "            row[\"choices\"] = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "            for param in bool_params:\n",
    "                if row[param].lower() == \"true\":\n",
    "                    row[param] = True\n",
    "                elif row[param].lower() == \"false\":\n",
    "                    row[param] = False\n",
    "                else:\n",
    "                    raise TypeError(f\"{param} data cannot be recognized\")\n",
    "\n",
    "            data_list.append(row)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def load_all_rephrase_data(split, dir_path, file_name):\n",
    "    data = {}\n",
    "    \n",
    "    for s in split:\n",
    "        file_path = f\"{dir_path}/raw_{s}{file_name}\"\n",
    "        # data[s] = load_csv_data(file_path, [])\n",
    "        # data[s] = load_csv_data(file_path, [\"concept\", \"name\", \"option\"])\n",
    "        data[s] = load_csv_data(file_path, [\"su_id_decision\", \"id_concept_appearance\", \"su_concept_appearance\"])\n",
    "    return data\n",
    "\n",
    "def save_data(samples, file_path):\n",
    "    # Get the keys from the first dictionary\n",
    "    header = samples[0].keys()\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(file_path, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the data\n",
    "        for row in samples:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f'CSV file \"{file_path}\" has been created with the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "minilm_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "multilingual_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def compute_similarity(text1, text2, multilingual=False):\n",
    "    if multilingual:\n",
    "        model = multilingual_model\n",
    "    else:\n",
    "        model = minilm_model\n",
    "\n",
    "    embeddings1 = model.encode([text1], convert_to_tensor=True)\n",
    "    embeddings2 = model.encode([text2], convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    return float(cosine_scores[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from better_profanity import profanity\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "en_stemmer = PorterStemmer()\n",
    "id_stemmer =  StemmerFactory().create_stemmer()\n",
    "\n",
    "def get_input_text(item):\n",
    "    return \" \".join([item['question'], item['question_concept']] + item['choices']['text'])\n",
    "\n",
    "def filter_concept(text, lang=\"english\"):\n",
    "    # Step 1: Lowercase both question and question_concept\n",
    "    question = text[\"question\"].lower()\n",
    "    question_concept = text[\"question_concept\"].lower()\n",
    "    \n",
    "    # Step 2: Check if question_concept appears in question\n",
    "    if question_concept in question:\n",
    "        return True\n",
    "\n",
    "    # Step 3: If not, split and remove stopwords\n",
    "    if lang != \"sundanese\":\n",
    "        stop_words = stopwords.words(lang)\n",
    "    else:\n",
    "        stop_words = []\n",
    "\n",
    "    concept_words = question_concept.split()\n",
    "    concept_words = [word for word in concept_words if word not in stop_words]\n",
    "\n",
    "    # Check if any of the remaining words in question_concept appear in question\n",
    "    if any(word in question for word in concept_words):\n",
    "        return True\n",
    "\n",
    "    # Step 4: Stem words and check if any stem word appears in question\n",
    "    \n",
    "    if lang != \"sundanese\":\n",
    "        if lang == \"english\":\n",
    "            stemmer = en_stemmer\n",
    "        elif lang == \"indonesian\":\n",
    "            stemmer = id_stemmer\n",
    "        \n",
    "        question_stemmed = \" \".join(stemmer.stem(word) for word in question.split())\n",
    "        if any(word in question_stemmed for word in [stemmer.stem(w) for w in concept_words]):\n",
    "            return True\n",
    "\n",
    "    # Step 5: If none of the above conditions met, return False\n",
    "    return False\n",
    "\n",
    "def filter_profanity(text):\n",
    "    all_texts = get_input_text(text)\n",
    "    \n",
    "    return not profanity.contains_profanity(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id_concept_appearance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\data_filtering.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# en_data = load_all_rephrase_data(split, \"92123\", \"_rephrased_name_92123.csv\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m id_data \u001b[39m=\u001b[39m load_all_rephrase_data(split, \u001b[39m\"\u001b[39m\u001b[39mv3-gpt4-1106/id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m su_data \u001b[39m=\u001b[39m load_all_rephrase_data(split, \u001b[39m\"\u001b[39;49m\u001b[39mv3-gpt4-1106/su\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# id_en_data = load_all_rephrase_data(split, \"backtranslation/id_en\", \".csv\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# su_en_data = load_all_rephrase_data(split, \"backtranslation/su_en\", \".csv\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# su_id_data = load_all_rephrase_data(split, \"v3-gpt4-1106/su_id\", \".csv\")\u001b[39;00m\n",
      "\u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\data_filtering.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdir_path\u001b[39m}\u001b[39;00m\u001b[39m/raw_\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m# data[s] = load_csv_data(file_path, [])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# data[s] = load_csv_data(file_path, [\"concept\", \"name\", \"option\"])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     data[s] \u001b[39m=\u001b[39m load_csv_data(file_path, [\u001b[39m\"\u001b[39;49m\u001b[39msu_id_decision\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mid_concept_appearance\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msu_concept_appearance\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\data_filtering.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m row[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mliteral_eval(row[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m bool_params:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mif\u001b[39;00m row[param]\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         row[param] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/data_filtering.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39melif\u001b[39;00m row[param]\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id_concept_appearance'"
     ]
    }
   ],
   "source": [
    "split = [\"validation\", \"test\", \"train\"]\n",
    "# en_data = load_all_rephrase_data(split, \"92123\", \"_rephrased_name_92123.csv\")\n",
    "id_data = load_all_rephrase_data(split, \"v3-gpt4-1106/id\", \".csv\")\n",
    "su_data = load_all_rephrase_data(split, \"v3-gpt4-1106/su\", \".csv\")\n",
    "\n",
    "# id_en_data = load_all_rephrase_data(split, \"backtranslation/id_en\", \".csv\")\n",
    "# su_en_data = load_all_rephrase_data(split, \"backtranslation/su_en\", \".csv\")\n",
    "# su_id_data = load_all_rephrase_data(split, \"v3-gpt4-1106/su_id\", \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering validation split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:17, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Erased Concept: 5\n",
      "SU Erased Concept: 68\n",
      "SU-ID Erased Threshold 0.9: 60\n",
      "CSV file \"./v3-gpt4-1106/id/raw_validation.csv\" has been created with the data.\n",
      "CSV file \"./v3-gpt4-1106/su/raw_validation.csv\" has been created with the data.\n",
      "\n",
      "Filtering test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [00:14, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Erased Concept: 3\n",
      "SU Erased Concept: 65\n",
      "SU-ID Erased Threshold 0.9: 67\n",
      "CSV file \"./v3-gpt4-1106/id/raw_test.csv\" has been created with the data.\n",
      "CSV file \"./v3-gpt4-1106/su/raw_test.csv\" has been created with the data.\n",
      "\n",
      "Filtering train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2162it [02:18, 15.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Erased Concept: 61\n",
      "SU Erased Concept: 552\n",
      "SU-ID Erased Threshold 0.9: 437\n",
      "CSV file \"./v3-gpt4-1106/id/raw_train.csv\" has been created with the data.\n",
      "CSV file \"./v3-gpt4-1106/su/raw_train.csv\" has been created with the data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# id_en_threshold = 0.9\n",
    "# su_en_threshold = 0.85\n",
    "su_id_threshold = 0.9\n",
    "\n",
    "for s in split:\n",
    "    print(f\"Filtering {s} split\")\n",
    "\n",
    "    id_count = 0\n",
    "    su_count = 0\n",
    "    # en_count = 0\n",
    "    # prof_count = 0\n",
    "\n",
    "    # id_en_count = 0\n",
    "    # su_en_count = 0\n",
    "    su_id_count = 0\n",
    "\n",
    "    for idx, item in tqdm(enumerate(id_data[s])):\n",
    "        # id_en = compute_similarity(get_input_text(item), get_input_text(id_en_data[s][idx]))\n",
    "        # su_en = compute_similarity(get_input_text(item), get_input_text(su_en_data[s][idx]))\n",
    "        su_id = compute_similarity(get_input_text(item), get_input_text(su_id_data[s][idx]), multilingual=True)\n",
    "        \n",
    "        id_ca = filter_concept(item, lang=\"indonesian\")\n",
    "        su_ca = filter_concept(su_data[s][idx], lang=\"sundanese\")\n",
    "\n",
    "        # id_data[s][idx][\"id_en_similarity\"] = id_en\n",
    "        # id_data[s][idx][\"id_en_decision\"] = bool(id_en >= id_en_threshold)\n",
    "        # id_data[s][idx][\"concept_appearance\"] = id_ca\n",
    "\n",
    "        # su_data[s][idx][\"su_en_similarity\"] = su_en\n",
    "        # su_data[s][idx][\"su_en_decision\"] = bool(su_en >= su_en_threshold)\n",
    "        su_data[s][idx][\"su_id_similarity\"] = su_id\n",
    "        su_data[s][idx][\"su_id_decision\"] = bool(su_id >= su_id_threshold)\n",
    "        su_data[s][idx][\"concept_appearance\"] = su_ca\n",
    "\n",
    "        # item[\"id_en_similarity\"] = id_en\n",
    "        # item[\"id_en_decision\"] = bool(id_en >= id_en_threshold)\n",
    "        item[\"id_concept_appearance\"] = id_ca\n",
    "        # item[\"su_en_similarity\"] = su_en\n",
    "        # item[\"su_en_decision\"] = bool(su_en >= su_en_threshold)\n",
    "        item[\"su_id_similarity\"] = su_id\n",
    "        item[\"su_id_decision\"] = bool(su_id >= su_id_threshold)\n",
    "        item[\"su_concept_appearance\"] = su_ca\n",
    "        # item[\"concept_appearance\"] = filter_concept(item)\n",
    "        # item[\"not_contain_profanity\"] = filter_profanity(item)\n",
    "        \n",
    "        # if not item[\"concept_appearance\"]:\n",
    "        #     en_count += 1\n",
    "        if not item[\"id_concept_appearance\"]:\n",
    "            id_count += 1\n",
    "        if not item[\"su_concept_appearance\"]:\n",
    "            su_count += 1\n",
    "        # if not item[\"not_contain_profanity\"]:\n",
    "        #     prof_count += 1\n",
    "        \n",
    "        # if id_en < id_en_threshold:\n",
    "        #     id_en_count += 1\n",
    "        # if su_en < su_en_threshold:\n",
    "        #     su_en_count += 1\n",
    "        if su_id < su_id_threshold:\n",
    "            su_id_count += 1\n",
    "        \n",
    "    # print(f\"EN Filtered Concept: {en_count}\")\n",
    "    print(f\"ID Erased Concept: {id_count}\")\n",
    "    print(f\"SU Erased Concept: {su_count}\")\n",
    "    # print(f\"ID-EN Filtered Threshold {id_en_threshold}: {id_en_count}\")\n",
    "    # print(f\"SU-EN Filtered Threshold {su_en_threshold}: {su_en_count}\")\n",
    "    print(f\"SU-ID Erased Threshold {su_id_threshold}: {su_id_count}\")\n",
    "    # print(f\"Filtered Profanity: {prof_count}\")\n",
    "\n",
    "    # save_data(en_data[s], f\"./filtered_data/en/{s}.csv\")\n",
    "    save_data(id_data[s], f\"./v3-gpt4-1106/id/raw_{s}.csv\")\n",
    "    save_data(su_data[s], f\"./v3-gpt4-1106/su/raw_{s}.csv\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '90b30172e645ff91f7171a048582eb8b', 'question': 'Rumah susun tersebut sulit dijual oleh agen properti, karena tepat berada di samping gedung apa?', 'question_concept': 'rumah susun', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['perkembangan pinggiran kota', 'gedung apartemen', 'halte bus', 'jakarta', 'pinggiran kota']}, 'answerKey': 'B', 'concept': 'False', 'name': 'False', 'option': 'True', 'id_concept_appearance': True, 'su_id_similarity': '0.8857952356338501', 'su_id_decision': False, 'su_concept_appearance': False}\n"
     ]
    }
   ],
   "source": [
    "print(id_data[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '90b30172e645ff91f7171a048582eb8b', 'question': 'Flat ieu hésé pikeun agén properti pikeun dijual, sabab caket sareng gedong naon?', 'question_concept': 'rumah susun', 'choices': {'label': ['A', 'B', 'C', 'D', 'E'], 'text': ['pangwangunan suburban', 'wangunan apartemen', 'eureun beus', 'Jakarta', 'suburbs']}, 'answerKey': 'B', 'su_id_similarity': '0.8857952356338501', 'su_id_decision': 'False', 'concept_appearance': 'False'}\n"
     ]
    }
   ],
   "source": [
    "print(su_data[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 274477.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Remaining Data for validation: 163\n",
      "CSV file \"./v3-gpt4-1106/id/filtered_validation.csv\" has been created with the data.\n",
      "CSV file \"./v3-gpt4-1106/su/filtered_validation.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [00:00, 235175.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Remaining Data for test: 130\n",
      "CSV file \"./v3-gpt4-1106/id/filtered_test.csv\" has been created with the data.\n",
      "CSV file \"./v3-gpt4-1106/su/filtered_test.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2162it [00:00, 432513.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Remaining Data for train: 1299\n",
      "CSV file \"./v3-gpt4-1106/id/filtered_train.csv\" has been created with the data.\n",
      "CSV file \"./v3-gpt4-1106/su/filtered_train.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in split:\n",
    "    # en_filtered = []\n",
    "    id_filtered = []\n",
    "    su_filtered = []\n",
    "    for idx, item in tqdm(enumerate(id_data[s])):\n",
    "        if item[\"su_id_decision\"] and item[\"id_concept_appearance\"] and item[\"su_concept_appearance\"]: # and item[\"not_contain_profanity\"] and item[\"su_en_decision\"] and item[\"id_en_decision\"] and item[\"concept_appearance\"]:\n",
    "            id_filtered.append({\n",
    "                'id': item['id'],\n",
    "                'question': item['question'],\n",
    "                'question_concept': item['question_concept'],\n",
    "                'choices': {\n",
    "                    'label': item['choices']['label'],\n",
    "                    'text': item['choices']['text']\n",
    "                },\n",
    "                'answerKey': item['answerKey']\n",
    "            })\n",
    "            # id_filtered.append({\n",
    "            #     'id': id_data[s][idx]['id'],\n",
    "            #     'question': id_data[s][idx]['question'],\n",
    "            #     'question_concept': id_data[s][idx]['question_concept'],\n",
    "            #     'choices': {\n",
    "            #         'label': id_data[s][idx]['choices']['label'],\n",
    "            #         'text': id_data[s][idx]['choices']['text']\n",
    "            #     },\n",
    "            #     'answerKey': id_data[s][idx]['answerKey']\n",
    "            # })\n",
    "            su_filtered.append({\n",
    "                'id': su_data[s][idx]['id'],\n",
    "                'question': su_data[s][idx]['question'],\n",
    "                'question_concept': su_data[s][idx]['question_concept'],\n",
    "                'choices': {\n",
    "                    'label': su_data[s][idx]['choices']['label'],\n",
    "                    'text': su_data[s][idx]['choices']['text']\n",
    "                },\n",
    "                'answerKey': su_data[s][idx]['answerKey']\n",
    "            })\n",
    "    print(f\"Count Remaining Data for {s}: {len(id_filtered)}\")\n",
    "    # save_data(en_filtered, f\"./filtered_data/en/{s}.csv\")\n",
    "    save_data(id_filtered, f\"./v3-gpt4-1106/id/filtered_{s}.csv\")\n",
    "    save_data(su_filtered, f\"./v3-gpt4-1106/su/filtered_{s}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
