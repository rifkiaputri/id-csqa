{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, bool_params):\n",
    "    # Initialize an empty list to store the data\n",
    "    data_list = []\n",
    "\n",
    "    # Open the CSV file for reading\n",
    "    with open(file_path, newline='', encoding=\"utf-8\") as csvfile:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Append the row (as a dictionary) to the data_list\n",
    "            row[\"choices\"] = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "            for param in bool_params:\n",
    "                if row[param].lower() == \"true\":\n",
    "                    row[param] = True\n",
    "                elif row[param].lower() == \"false\":\n",
    "                    row[param] = False\n",
    "                else:\n",
    "                    raise TypeError(f\"{param} data cannot be recognized\")\n",
    "\n",
    "            data_list.append(row)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def load_all_rephrase_data(split, dir_path, file_name):\n",
    "    data = {}\n",
    "    \n",
    "    for s in split:\n",
    "        file_path = f\"{dir_path}/{s}{file_name}\"\n",
    "        data[s] = load_csv_data(file_path, [])\n",
    "        # data[s] = load_csv_data(file_path, [\"concept\", \"name\", \"option\"])\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_data(samples, file_path):\n",
    "    # Get the keys from the first dictionary\n",
    "    header = samples[0].keys()\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(file_path, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the data\n",
    "        for row in samples:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f'CSV file \"{file_path}\" has been created with the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "minilm_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "multilingual_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def compute_similarity(text1, text2, multilingual=False):\n",
    "    if multilingual:\n",
    "        model = multilingual_model\n",
    "    else:\n",
    "        model = minilm_model\n",
    "\n",
    "    embeddings1 = model.encode([text1], convert_to_tensor=True)\n",
    "    embeddings2 = model.encode([text2], convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "    return float(cosine_scores[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from better_profanity import profanity\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "en_stemmer = PorterStemmer()\n",
    "id_stemmer =  StemmerFactory().create_stemmer()\n",
    "\n",
    "def get_input_text(item):\n",
    "    return \" \".join([item['question'], item['question_concept']] + item['choices']['text'])\n",
    "\n",
    "def filter_concept(text, lang=\"english\"):\n",
    "    # Step 1: Lowercase both question and question_concept\n",
    "    question = text[\"question\"].lower()\n",
    "    question_concept = text[\"question_concept\"].lower()\n",
    "    \n",
    "    # Step 2: Check if question_concept appears in question\n",
    "    if question_concept in question:\n",
    "        return True\n",
    "\n",
    "    # Step 3: If not, split and remove stopwords\n",
    "    if lang != \"sundanese\":\n",
    "        stop_words = stopwords.words(lang)\n",
    "    else:\n",
    "        stop_words = []\n",
    "\n",
    "    concept_words = question_concept.split()\n",
    "    concept_words = [word for word in concept_words if word not in stop_words]\n",
    "\n",
    "    # Check if any of the remaining words in question_concept appear in question\n",
    "    if any(word in question for word in concept_words):\n",
    "        return True\n",
    "\n",
    "    # Step 4: Stem words and check if any stem word appears in question\n",
    "    \n",
    "    if lang != \"sundanese\":\n",
    "        if lang == \"english\":\n",
    "            stemmer = en_stemmer\n",
    "        elif lang == \"indonesian\":\n",
    "            stemmer = id_stemmer\n",
    "        \n",
    "        question_stemmed = \" \".join(stemmer.stem(word) for word in question.split())\n",
    "        if any(word in question_stemmed for word in [stemmer.stem(w) for w in concept_words]):\n",
    "            return True\n",
    "\n",
    "    # Step 5: If none of the above conditions met, return False\n",
    "    return False\n",
    "\n",
    "def filter_profanity(text):\n",
    "    all_texts = get_input_text(text)\n",
    "    \n",
    "    return not profanity.contains_profanity(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = [\"validation\", \"test\", \"train\"]\n",
    "en_data = load_all_rephrase_data(split, \"92123\", \"_rephrased_name_92123.csv\")\n",
    "id_data = load_all_rephrase_data(split, \"translated_data/id\", \".csv\")\n",
    "su_data = load_all_rephrase_data(split, \"translated_data/su\", \".csv\")\n",
    "\n",
    "id_en_data = load_all_rephrase_data(split, \"backtranslation/id_en\", \".csv\")\n",
    "su_en_data = load_all_rephrase_data(split, \"backtranslation/su_en\", \".csv\")\n",
    "su_id_data = load_all_rephrase_data(split, \"backtranslation/su_id\", \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering validation split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [01:01,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN Filtered Concept: 14\n",
      "ID Filtered Concept: 58\n",
      "SU Filtered Concept: 100\n",
      "ID-EN Filtered Threshold 0.9: 63\n",
      "SU-EN Filtered Threshold 0.85: 71\n",
      "SU-ID Filtered Threshold 0.9: 44\n",
      "Filtered Profanity: 14\n",
      "CSV file \"./filtered_data/en/validation.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/id/validation.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/su/validation.csv\" has been created with the data.\n",
      "\n",
      "Filtering test split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [00:51,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN Filtered Concept: 17\n",
      "ID Filtered Concept: 62\n",
      "SU Filtered Concept: 98\n",
      "ID-EN Filtered Threshold 0.9: 51\n",
      "SU-EN Filtered Threshold 0.85: 60\n",
      "SU-ID Filtered Threshold 0.9: 30\n",
      "Filtered Profanity: 10\n",
      "CSV file \"./filtered_data/en/test.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/id/test.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/su/test.csv\" has been created with the data.\n",
      "\n",
      "Filtering train split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2162it [07:48,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN Filtered Concept: 104\n",
      "ID Filtered Concept: 449\n",
      "SU Filtered Concept: 791\n",
      "ID-EN Filtered Threshold 0.9: 511\n",
      "SU-EN Filtered Threshold 0.85: 509\n",
      "SU-ID Filtered Threshold 0.9: 288\n",
      "Filtered Profanity: 86\n",
      "CSV file \"./filtered_data/en/train.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/id/train.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/su/train.csv\" has been created with the data.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "id_en_threshold = 0.9\n",
    "su_en_threshold = 0.85\n",
    "su_id_threshold = 0.9\n",
    "\n",
    "for s in split:\n",
    "    print(f\"Filtering {s} split\")\n",
    "\n",
    "    id_count = 0\n",
    "    su_count = 0\n",
    "    en_count = 0\n",
    "    prof_count = 0\n",
    "\n",
    "    id_en_count = 0\n",
    "    su_en_count = 0\n",
    "    su_id_count = 0\n",
    "\n",
    "    for idx, item in tqdm(enumerate(en_data[s])):\n",
    "        id_en = compute_similarity(get_input_text(item), get_input_text(id_en_data[s][idx]))\n",
    "        su_en = compute_similarity(get_input_text(item), get_input_text(su_en_data[s][idx]))\n",
    "        su_id = compute_similarity(get_input_text(id_data[s][idx]), get_input_text(su_id_data[s][idx]), multilingual=True)\n",
    "        \n",
    "        id_ca = filter_concept(id_data[s][idx], lang=\"indonesian\")\n",
    "        su_ca = filter_concept(su_data[s][idx], lang=\"sundanese\")\n",
    "\n",
    "        id_data[s][idx][\"id_en_similarity\"] = id_en\n",
    "        id_data[s][idx][\"id_en_decision\"] = bool(id_en >= id_en_threshold)\n",
    "        id_data[s][idx][\"concept_appearance\"] = id_ca\n",
    "\n",
    "        su_data[s][idx][\"su_en_similarity\"] = su_en\n",
    "        su_data[s][idx][\"su_en_decision\"] = bool(su_en >= su_en_threshold)\n",
    "        su_data[s][idx][\"su_id_similarity\"] = su_id\n",
    "        su_data[s][idx][\"su_id_decision\"] = bool(su_id >= su_id_threshold)\n",
    "        su_data[s][idx][\"concept_appearance\"] = su_ca\n",
    "\n",
    "        item[\"id_en_similarity\"] = id_en\n",
    "        item[\"id_en_decision\"] = bool(id_en >= id_en_threshold)\n",
    "        item[\"id_concept_appearance\"] = id_ca\n",
    "        item[\"su_en_similarity\"] = su_en\n",
    "        item[\"su_en_decision\"] = bool(su_en >= su_en_threshold)\n",
    "        item[\"su_id_similarity\"] = su_id\n",
    "        item[\"su_id_decision\"] = bool(su_id >= su_id_threshold)\n",
    "        item[\"su_concept_appearance\"] = su_ca\n",
    "        item[\"concept_appearance\"] = filter_concept(item)\n",
    "        item[\"not_contain_profanity\"] = filter_profanity(item)\n",
    "        \n",
    "        if not item[\"concept_appearance\"]:\n",
    "            en_count += 1\n",
    "        if not item[\"id_concept_appearance\"]:\n",
    "            id_count += 1\n",
    "        if not item[\"su_concept_appearance\"]:\n",
    "            su_count += 1\n",
    "        if not item[\"not_contain_profanity\"]:\n",
    "            prof_count += 1\n",
    "        \n",
    "        if id_en < id_en_threshold:\n",
    "            id_en_count += 1\n",
    "        if su_en < su_en_threshold:\n",
    "            su_en_count += 1\n",
    "        if su_id < su_id_threshold:\n",
    "            su_id_count += 1\n",
    "        \n",
    "    print(f\"EN Filtered Concept: {en_count}\")\n",
    "    print(f\"ID Filtered Concept: {id_count}\")\n",
    "    print(f\"SU Filtered Concept: {su_count}\")\n",
    "    print(f\"ID-EN Filtered Threshold {id_en_threshold}: {id_en_count}\")\n",
    "    print(f\"SU-EN Filtered Threshold {su_en_threshold}: {su_en_count}\")\n",
    "    print(f\"SU-ID Filtered Threshold {su_id_threshold}: {su_id_count}\")\n",
    "    print(f\"Filtered Profanity: {prof_count}\")\n",
    "\n",
    "    save_data(en_data[s], f\"./filtered_data/en/{s}.csv\")\n",
    "    save_data(id_data[s], f\"./filtered_data/id/{s}.csv\")\n",
    "    save_data(su_data[s], f\"./filtered_data/su/{s}.csv\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 136830.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Filtered Data for validation: 101\n",
      "CSV file \"./filtered_data/en/validation.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/id/validation.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/su/validation.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "236it [00:00, 236242.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Filtered Data for test: 82\n",
      "CSV file \"./filtered_data/en/test.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/id/test.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/su/test.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2162it [00:00, 254207.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Filtered Data for train: 840\n",
      "CSV file \"./filtered_data/en/train.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/id/train.csv\" has been created with the data.\n",
      "CSV file \"./filtered_data/su/train.csv\" has been created with the data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in split:\n",
    "    en_filtered = []\n",
    "    id_filtered = []\n",
    "    su_filtered = []\n",
    "    for idx, item in tqdm(enumerate(en_data[s])):\n",
    "        if item[\"id_en_decision\"] and item[\"su_en_decision\"] and item[\"su_id_decision\"] and item[\"concept_appearance\"] and item[\"id_concept_appearance\"] and item[\"su_concept_appearance\"] and item[\"not_contain_profanity\"]:\n",
    "            en_filtered.append({\n",
    "                'id': item['id'],\n",
    "                'question': item['question'],\n",
    "                'question_concept': item['question_concept'],\n",
    "                'choices': {\n",
    "                    'label': item['choices']['label'],\n",
    "                    'text': item['choices']['text']\n",
    "                },\n",
    "                'answerKey': item['answerKey']\n",
    "            })\n",
    "            id_filtered.append({\n",
    "                'id': id_data[s][idx]['id'],\n",
    "                'question': id_data[s][idx]['question'],\n",
    "                'question_concept': id_data[s][idx]['question_concept'],\n",
    "                'choices': {\n",
    "                    'label': id_data[s][idx]['choices']['label'],\n",
    "                    'text': id_data[s][idx]['choices']['text']\n",
    "                },\n",
    "                'answerKey': id_data[s][idx]['answerKey']\n",
    "            })\n",
    "            su_filtered.append({\n",
    "                'id': su_data[s][idx]['id'],\n",
    "                'question': su_data[s][idx]['question'],\n",
    "                'question_concept': su_data[s][idx]['question_concept'],\n",
    "                'choices': {\n",
    "                    'label': su_data[s][idx]['choices']['label'],\n",
    "                    'text': su_data[s][idx]['choices']['text']\n",
    "                },\n",
    "                'answerKey': su_data[s][idx]['answerKey']\n",
    "            })\n",
    "    print(f\"Count Filtered Data for {s}: {len(en_filtered)}\")\n",
    "    save_data(en_filtered, f\"./filtered_data/en/{s}.csv\")\n",
    "    save_data(id_filtered, f\"./filtered_data/id/{s}.csv\")\n",
    "    save_data(su_filtered, f\"./filtered_data/su/{s}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
