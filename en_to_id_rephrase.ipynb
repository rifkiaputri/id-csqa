{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import time\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"API_KEY\")\n",
    "org_key = os.environ.get(\"ORG_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV Rephrase Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "split = [\"train\"]\n",
    "\n",
    "all_data = {}\n",
    "\n",
    "for s in split:\n",
    "    csv_file_path = f'{s}_rephrase.csv'\n",
    "\n",
    "    # Initialize an empty list to store the data\n",
    "    data_list = []\n",
    "\n",
    "    # Open the CSV file for reading\n",
    "    with open(csv_file_path, newline='') as csvfile:\n",
    "        # Create a CSV reader object\n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            # Append the row (as a dictionary) to the data_list\n",
    "            row[\"choices\"] = ast.literal_eval(row[\"choices\"])\n",
    "\n",
    "            rephrase_params = [\"concept\", \"name\", \"option\"]\n",
    "            for param in rephrase_params:\n",
    "                if row[param] == \"True\":\n",
    "                    row[param] = True\n",
    "                elif row[param] == \"False\":\n",
    "                    row[param] = False\n",
    "                else:\n",
    "                    raise TypeError(f\"{param} data cannot be recognized\")\n",
    "\n",
    "            data_list.append(row)\n",
    "    \n",
    "    all_data[s] = data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate choice text\n",
    "def generate_choices_text(choices):\n",
    "    labels = choices[\"label\"]\n",
    "    texts = choices[\"text\"]\n",
    "\n",
    "    choice_text = \"\"\n",
    "    for idx, label in enumerate(labels):\n",
    "        choice_text += f'{label}. \"{texts[idx]}\"\\n'\n",
    "    \n",
    "    return choice_text\n",
    "\n",
    "# Function to generate answer text\n",
    "def generate_answer_text(choices, answerKey):\n",
    "    idx = choices[\"label\"].index(answerKey)\n",
    "    answer_text = f'{answerKey}. \"{choices[\"text\"][idx]}\"'\n",
    "    \n",
    "    return answer_text\n",
    "\n",
    "# Function to generate prompts based on the conditions\n",
    "def generate_rephrase_name_prompt(row):\n",
    "    return f\"\"\"Change all names in the given phrases to Indonesian names. Change only the names. Keep all remaining phrases and keep it all in english and reply with only your answer.\n",
    "\n",
    "Phrase: {row['question']}\n",
    "Answer:\"\"\"\n",
    "\n",
    "def generate_rephrase_all_prompt(row):\n",
    "    return f\"\"\"Given a commonsense question, a concept, options, and the question answer, change them to become relevant to Indonesia. If an aspect is flagged to be changed, then you need to change it completely. If it's flagged as keep, then keep as it is. Make sure your changes are still in the same domain/topic with the given data, and there is only one clear answer in the options. Reply with only your changed data in a JSON format.\n",
    "\n",
    "Data:\n",
    "###\n",
    "Question: {row['question']} -> Change\n",
    "Concept: {row['question_concept']} -> {'Change' if row['concept'] else 'Keep'}\n",
    "Options: -> {'Change' if row['option'] else 'Keep'}\n",
    "{generate_choices_text(row['choices'])}Question Answer: {generate_answer_text(row['choices'], row['answerKey']) if row['answerKey'] else ''}\n",
    "###\n",
    "\n",
    "Changed data:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rephrase Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_chat_completion(input_prompt, model_name):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': input_prompt \n",
    "            }\n",
    "        ],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "# Define a function to rephrase the CSV data using OpenAI GPT-3.5-Turbo\n",
    "def rephrase_csv_data(row, model_name, history):\n",
    "    if row[\"name\"] and not row[\"concept\"] and not row[\"option\"]:\n",
    "        input_prompt = generate_rephrase_name_prompt(row)\n",
    "    else:\n",
    "        input_prompt = generate_rephrase_all_prompt(row)\n",
    "\n",
    "    if input_prompt in history.keys():\n",
    "        return input_prompt, history[input_prompt][\"response\"]\n",
    "\n",
    "    try:\n",
    "        completion = get_openai_chat_completion(input_prompt, model_name)\n",
    "    except Exception:\n",
    "        print('Caught exception, wait for 1 min...')\n",
    "        time.sleep(60)\n",
    "        completion = get_openai_chat_completion(input_prompt, model_name)\n",
    "    \n",
    "    response = completion.choices[0].message.content.strip()\n",
    "            \n",
    "    return input_prompt, response\n",
    "\n",
    "def postprocess_result(row, response, split):\n",
    "    if row[\"name\"] and not row[\"concept\"] and not row[\"option\"]:\n",
    "        if \"Answer: \" in response:\n",
    "            rephrased_result = response.split(\"Answer: \")[-1]\n",
    "        else:\n",
    "            rephrased_result = response\n",
    "    else:\n",
    "        rephrased_result = ast.literal_eval(response)\n",
    "        letters = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "        if isinstance(rephrased_result[\"Options\"], list):\n",
    "            if len(rephrased_result[\"Options\"]) == 5:\n",
    "                rephrased_result[\"Options\"] = dict(zip(letters, rephrased_result[\"Options\"]))\n",
    "            else:\n",
    "                option_labels = [option[0] for option in rephrased_result[\"Options\"]]\n",
    "                if set(option_labels).issubset(set(letters)):\n",
    "                    option_texts = [option[4:] for option in rephrased_result[\"Options\"]]\n",
    "                    label_not_in_option = list(set(letters) - set(option_labels))\n",
    "                    for label in label_not_in_option:\n",
    "                        chosen_idx = row[\"choices\"][\"label\"].index(label)\n",
    "                        chosen_text = row[\"choices\"][\"text\"][chosen_idx]\n",
    "                        option_texts.insert(chosen_idx, chosen_text)\n",
    "\n",
    "                    rephrased_result[\"Options\"] = dict(zip(letters, option_texts))\n",
    "                else:\n",
    "                    raise ValueError(f\"Option output is not right: {rephrased_result['Options']}\")\n",
    "        \n",
    "        if split != \"test\":\n",
    "            if rephrased_result[\"Question Answer\"][0] not in letters:\n",
    "                option_texts = [text.lower() for text in list(rephrased_result[\"Options\"].values())]\n",
    "                if rephrased_result['Question Answer'].lower() in option_texts:\n",
    "                    option_labels = list(rephrased_result[\"Options\"].keys())\n",
    "                    answer_idx = option_texts.index(rephrased_result['Question Answer'].lower())\n",
    "                    rephrased_result[\"Question Answer\"] = option_labels[answer_idx]\n",
    "                else:\n",
    "                    raise ValueError(f\"Answer key not in options: {rephrased_result['Question Answer']}\")\n",
    "            else:\n",
    "                rephrased_result[\"Question Answer\"] = rephrased_result[\"Question Answer\"][0]\n",
    "    \n",
    "    return rephrased_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Rephrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_key\n",
    "openai.organization = org_key\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data on split: train\n",
      "Load response history from file train_history_91123.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1196/2162 [01:33<01:15, 12.73it/s]\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function get_openai_chat_completion at 0x0000017D845C8CA0>: it's not the same object as __main__.get_openai_chat_completion",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\en_to_id_rephrase.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(all_data[s]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     rephrased_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     prompt, response \u001b[39m=\u001b[39m rephrase_csv_data(data, model_name, response_history)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     response_history[prompt] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m: response}\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     resp_history_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m: response_history\u001b[39m.\u001b[39mkeys(), \u001b[39m'\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m'\u001b[39m: response_history\u001b[39m.\u001b[39mvalues()})\n",
      "\u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\en_to_id_rephrase.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m input_prompt, history[input_prompt][\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     completion \u001b[39m=\u001b[39m get_openai_chat_completion(input_prompt, model_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mexcept\u001b[39;00m timeout_decorator\u001b[39m.\u001b[39mTimeoutError:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/My%20Drive/rn/KAIST/College/Project/Indo%20Commonsense%20QA/id-csqa/en_to_id_rephrase.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCaught exception, wait for 1 min...\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\.venv\\lib\\site-packages\\timeout_decorator\\timeout_decorator.py:92\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorate.<locals>.new_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39m@wraps\u001b[39m(function)\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_function\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     91\u001b[0m     timeout_wrapper \u001b[39m=\u001b[39m _Timeout(function, timeout_exception, exception_message, seconds)\n\u001b[1;32m---> 92\u001b[0m     \u001b[39mreturn\u001b[39;00m timeout_wrapper(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mg:\\My Drive\\rn\\KAIST\\College\\Project\\Indo Commonsense QA\\id-csqa\\.venv\\lib\\site-packages\\timeout_decorator\\timeout_decorator.py:147\u001b[0m, in \u001b[0;36m_Timeout.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__process \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39mProcess(target\u001b[39m=\u001b[39m_target,\n\u001b[0;32m    144\u001b[0m                                          args\u001b[39m=\u001b[39margs,\n\u001b[0;32m    145\u001b[0m                                          kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__process\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__process\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__limit \u001b[39m+\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function get_openai_chat_completion at 0x0000017D845C8CA0>: it's not the same object as __main__.get_openai_chat_completion"
     ]
    }
   ],
   "source": [
    "for s in split:\n",
    "    print(f\"Process data on split: {s}\")\n",
    "    \n",
    "    history_path = f\"{s}_history_91123.csv\"\n",
    "    if os.path.exists(history_path):\n",
    "        print(f\"Load response history from file {history_path}\")\n",
    "        resp_history_df = pd.read_csv(history_path, converters={'response': lambda x: ast.literal_eval(x)})\n",
    "        response_history = dict(zip(resp_history_df.prompt, resp_history_df.response))\n",
    "    else:\n",
    "        print(f\"Initialize response history\")\n",
    "        response_history = {}\n",
    "\n",
    "    rephrased_results = []\n",
    "    for data in tqdm(all_data[s]):\n",
    "        rephrased_data = data.copy()\n",
    "        \n",
    "        prompt, response = rephrase_csv_data(data, model_name, response_history)\n",
    "        response_history[prompt] = {\"response\": response}\n",
    "        \n",
    "        resp_history_df = pd.DataFrame({'prompt': response_history.keys(), 'response': response_history.values()})\n",
    "        resp_history_df.to_csv(history_path, index=False)\n",
    "\n",
    "        result = postprocess_result(data, response, s)\n",
    "        if isinstance(result, dict):\n",
    "            rephrased_data[\"question\"] = result[\"Question\"]\n",
    "            rephrased_data[\"choices\"] = {\n",
    "                \"label\": list(result[\"Options\"].keys()),\n",
    "                \"text\": [text.lower() for text in list(result[\"Options\"].values())],\n",
    "            }\n",
    "            if \"Concept\" in list(result.keys()):\n",
    "                rephrased_data[\"question_concept\"] = result[\"Concept\"]\n",
    "            if s != \"test\":\n",
    "                rephrased_data[\"answerKey\"] = result[\"Question Answer\"]\n",
    "        elif isinstance(result, str):\n",
    "            rephrased_data[\"question\"] = result\n",
    "        \n",
    "        rephrased_results.append(rephrased_data)\n",
    "\n",
    "    # Specify the path to the CSV file\n",
    "    rephrased_file_path = f'{s}_rephrased_91123.csv'\n",
    "\n",
    "    # Get the keys from the first dictionary\n",
    "    header = rephrased_results[0].keys()\n",
    "\n",
    "    # Write the data to the CSV file\n",
    "    with open(rephrased_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the data\n",
    "        for row in rephrased_results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f'CSV file \"{rephrased_file_path}\" has been created with the data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
