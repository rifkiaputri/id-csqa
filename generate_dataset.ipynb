{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import stanza\n",
    "import time\n",
    "from nltk.corpus import wordnet as wn\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needs to be run one time\n",
    "# stanza.download('en', model_dir=os.environ['HF_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 15:25:53 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2023-08-24 15:25:53 INFO: Using device: 0\n",
      "2023-08-24 15:25:53 INFO: Loading: tokenize\n",
      "2023-08-24 15:25:54 INFO: Loading: pos\n",
      "2023-08-24 15:25:54 INFO: Loading: lemma\n",
      "2023-08-24 15:25:55 INFO: Loading: ner\n",
      "2023-08-24 15:25:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_pipeline = stanza.Pipeline('en', processors='tokenize,pos,lemma,ner', device=0, model_dir=os.environ['HF_HOME'], download_method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(f\"https://api.conceptnet.io/query?node=/c/en/body_part&limit=5000\")\n",
    "obj = res.json()\n",
    "body_parts = set()\n",
    "for edge in obj['edges']:\n",
    "    start_edge = edge['start']['@id'].replace('/c/en/', '')\n",
    "    if '/' not in start_edge:\n",
    "        body_parts.add(start_edge.replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_concepts = [\n",
    "    \"sex\", \"vagina\", \"penis\", \"prostitute\", \"kiss\", \"copulating\", \"copulate\", \"procreating\", \"procreate\",\n",
    "    \"killing people\", \"committing murder\", \"affair\", \"drug dealer\", \"terrorists\", \"terrorist\",\n",
    "]\n",
    "general_concepts = [\n",
    "    \"human\", \"animal\", \"plant\", \"thing\", \"everyone\", \"people\", \"person\"\n",
    "] + list(body_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senses(concept):\n",
    "    concept = '_'.join(concept.split())\n",
    "    synsets = wn.synsets(concept)\n",
    "    pos_tags = set()\n",
    "    for s in synsets:\n",
    "        pos_tags.add(s.pos())\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "def has_multi_sense(concept):\n",
    "    pos_tags = get_senses(concept)\n",
    "    return len(pos_tags) >= 2\n",
    "\n",
    "\n",
    "def extract_pos_by_word(sentence, word):\n",
    "    words = word.lower().split('_')\n",
    "    pos_tags = set()\n",
    "    results = nlp_pipeline(sentence)\n",
    "    for sent in results.sentences:\n",
    "        for s_word in sent.words:\n",
    "            if s_word.lemma in words or s_word.text.lower() in words:\n",
    "                pos_tags.add(s_word.upos)\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "filler_tags = [\n",
    "    wn.ADJ, wn.ADJ_SAT, wn.ADV,\n",
    "    'ADJ', 'ADP', 'ADV', 'ADP', 'CCONJ', 'SCONJ', 'DET', 'PART', 'PUNCT'\n",
    "]\n",
    "def is_filler(concept, sentence):\n",
    "    senses = get_senses(concept)\n",
    "    if len(senses) > 1:\n",
    "        senses = extract_pos_by_word(sentence, concept)\n",
    "\n",
    "    return all([s in filler_tags for s in senses])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept Relevancy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_options_lemma(text):\n",
    "    doc = nlp_pipeline(text)\n",
    "    lemmas = []\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            if word.lemma is None:\n",
    "                lemmas.append(word.text)\n",
    "            else:\n",
    "                lemmas.append(word.lemma)\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset commonsense_qa (/media/kiki/kiki_hdd/cache/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n",
      "100%|██████████| 9741/9741 [19:52<00:00,  8.17it/s]\n",
      "Found cached dataset commonsense_qa (/media/kiki/kiki_hdd/cache/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n",
      "100%|██████████| 1221/1221 [02:27<00:00,  8.29it/s]\n",
      "Found cached dataset commonsense_qa (/media/kiki/kiki_hdd/cache/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n",
      "100%|██████████| 1140/1140 [02:15<00:00,  8.41it/s]\n"
     ]
    }
   ],
   "source": [
    "out_parent_dir = \"../dataset/relevancy_context/\"\n",
    "for split_name in [\"train\", \"validation\", \"test\"]:\n",
    "    lang_name = \"id\"\n",
    "    en_csqa = load_dataset('commonsense_qa', split=split_name)\n",
    "    relevancy_df = pd.read_csv(f\"{out_parent_dir}{split_name}_{lang_name}_relevancy.csv\", index_col=0, converters={'q_concept': lambda x: x[1:-1].replace(\"'\", '').split(', ')})\n",
    "\n",
    "    general_questions, rephrase_questions = [], []\n",
    "    for item in tqdm(en_csqa):\n",
    "        q_id = item['id']\n",
    "\n",
    "        # Check q concept offensiveness\n",
    "        options_str = ' '.join(item['choices']['text']).lower()\n",
    "        is_offensive = any(ex in item['question'].lower() for ex in excluded_concepts) or \\\n",
    "            any(ex in options_str for ex in excluded_concepts)\n",
    "        if is_offensive:\n",
    "            continue\n",
    "\n",
    "        # Check options ambiguity\n",
    "        options_lemma = [get_options_lemma(option) for option in item['choices']['text']]\n",
    "        is_options_ambiguous = len(set(options_lemma)) != len(options_lemma)\n",
    "        if is_options_ambiguous:\n",
    "            continue\n",
    "\n",
    "        # Check name relevancy\n",
    "        is_name_irrelevant = relevancy_df.loc[q_id, 'names'] != \"[]\"\n",
    "\n",
    "        # Check options relevancy\n",
    "        options = [\n",
    "            relevancy_df.loc[q_id, 'option_a'], relevancy_df.loc[q_id, 'option_b'],\n",
    "            relevancy_df.loc[q_id, 'option_c'], relevancy_df.loc[q_id, 'option_d'],\n",
    "            relevancy_df.loc[q_id, 'option_e']\n",
    "        ]\n",
    "        is_option_irrelevant = 'no' in options\n",
    "\n",
    "        # Check q concept relevancy\n",
    "        # is_relevant_concept = relevancy_df.loc[q_id, 'q_concept'] == 'yes'\n",
    "        yes_count = relevancy_df.loc[q_id, 'q_concept'].count('yes')\n",
    "        no_count = relevancy_df.loc[q_id, 'q_concept'].count('no')\n",
    "        # is_relevant_concept = relevancy_df.loc[q_id, 'q_concept'].count('yes') >= 2\n",
    "\n",
    "        # Check concept generality\n",
    "        is_general_concept = any(c in item['question_concept'] for c in general_concepts) or is_filler(item['question_concept'], item['question'])\n",
    "        \n",
    "        if not is_name_irrelevant and not is_option_irrelevant and (is_general_concept or yes_count >= 3):\n",
    "            general_questions.append(item)\n",
    "        else:\n",
    "            if (not is_general_concept and no_count >= 3) or is_name_irrelevant or is_option_irrelevant:\n",
    "                item['concept'] = not is_general_concept and no_count >= 3\n",
    "                item['name'] = is_name_irrelevant\n",
    "                item['option'] = is_option_irrelevant\n",
    "                rephrase_questions.append(item)\n",
    "\n",
    "    q_df_general = pd.DataFrame(general_questions)\n",
    "    q_df_rephrase = pd.DataFrame(rephrase_questions)\n",
    "\n",
    "    q_df_general.to_csv(f\"{out_parent_dir}data_result/{split_name}_general.csv\", index=False)\n",
    "    q_df_rephrase.to_csv(f\"{out_parent_dir}data_result/{split_name}_rephrase.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stat = pd.DataFrame(index=np.array(['train', 'validation', 'test']), columns=['general', 'rephrase', 'irr_concept', 'irr_location', 'irr_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 148.57it/s]\n"
     ]
    }
   ],
   "source": [
    "out_parent_dir = \"../dataset/relevancy_context/data_result/\"\n",
    "for file_name in tqdm(os.listdir(out_parent_dir)):\n",
    "    name_only = file_name.replace('.csv', '')\n",
    "    split, q_type = name_only.split('_')\n",
    "    \n",
    "    dat = pd.read_csv(out_parent_dir + file_name)\n",
    "    data_stat.loc[split][q_type] = len(dat)\n",
    "\n",
    "    if q_type == 'rephrase':\n",
    "        dat['concept'].tolist()\n",
    "        dat['name'].tolist()\n",
    "        dat['option'].tolist()\n",
    "        data_stat.loc[split]['irr_concept'] = dat['concept'].tolist().count(True)\n",
    "        data_stat.loc[split]['irr_location'] = dat['option'].tolist().count(True)\n",
    "        data_stat.loc[split]['irr_names'] = dat['name'].tolist().count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>general</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>irr_concept</th>\n",
       "      <th>irr_location</th>\n",
       "      <th>irr_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>7140</td>\n",
       "      <td>2162</td>\n",
       "      <td>445</td>\n",
       "      <td>984</td>\n",
       "      <td>1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>882</td>\n",
       "      <td>274</td>\n",
       "      <td>68</td>\n",
       "      <td>125</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>841</td>\n",
       "      <td>236</td>\n",
       "      <td>51</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           general rephrase irr_concept irr_location irr_names\n",
       "train         7140     2162         445          984      1036\n",
       "validation     882      274          68          125       126\n",
       "test           841      236          51          108       109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "general         8863\n",
       "rephrase        2672\n",
       "irr_concept      564\n",
       "irr_location    1217\n",
       "irr_names       1271\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stat.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be run first before the above blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parent_dir = \"../dataset/relevancy_context/\"\n",
    "lang_name = \"id\"\n",
    "location_context = \"Indonesia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(sentence):\n",
    "    ner_results = nlp_pipeline(sentence)\n",
    "    return [ent.text for ent in ner_results.ents if ent.type == 'PERSON']\n",
    "\n",
    "\n",
    "def extract_locations(sentence):\n",
    "    ner_results = nlp_pipeline(sentence)\n",
    "    return [ent.text for ent in ner_results.ents if ent.type in ['GPE', 'LOC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location history found!\n",
      "Activity history found!\n"
     ]
    }
   ],
   "source": [
    "loc_history_filename =  f\"{out_parent_dir}conceptnet-api_location_history.csv\"\n",
    "activity_history_filename = f\"{out_parent_dir}conceptnet-api_activity_history.csv\"\n",
    "\n",
    "loc_history_path = Path(loc_history_filename)\n",
    "if loc_history_path.is_file():\n",
    "    print(\"Location history found!\")\n",
    "    loc_history_df = pd.read_csv(loc_history_filename)\n",
    "    loc_response_history = dict((k, v) for k, v in zip(loc_history_df.prompt, loc_history_df.response) if ' ' not in k)\n",
    "else:\n",
    "    print(\"Location history not found. Initializing new one...\")\n",
    "    loc_response_history = {}\n",
    "\n",
    "activity_history_path = Path(activity_history_filename)\n",
    "if activity_history_path.is_file():\n",
    "    print(\"Activity history found!\")\n",
    "    activity_history_df = pd.read_csv(activity_history_filename)\n",
    "    activity_response_history = dict((k, v) for k, v in zip(activity_history_df.prompt, activity_history_df.response) if ' ' not in k)\n",
    "else:\n",
    "    print(\"Activity history not found. Initializing new one...\")\n",
    "    activity_response_history = {}\n",
    "\n",
    "location_concepts = ['administrative_region', 'country', 'city', 'province']\n",
    "excluded = ['city', 'town', 'park', 'country', 'province', 'countryside', 'village']\n",
    "\n",
    "def is_conceptnet_location(option):\n",
    "    option = '_'.join(option.split())\n",
    "\n",
    "    if option in excluded:\n",
    "        return False\n",
    "\n",
    "    if option in loc_response_history:\n",
    "        return loc_response_history[option]\n",
    "    \n",
    "    res = requests.get(f\"https://api.conceptnet.io/query?node=/c/en/{option}&rel=/r/IsA\")\n",
    "    obj = res.json()\n",
    "    is_location = False\n",
    "    for edge in obj['edges']:\n",
    "        end_edge = edge['end']['@id'].split('/')\n",
    "        if any([e in end_edge for e in location_concepts]):\n",
    "            is_location = True\n",
    "            break\n",
    "\n",
    "    loc_response_history[option] = is_location\n",
    "    resp_history_df = pd.DataFrame({'prompt': loc_response_history.keys(), 'response': loc_response_history.values()})\n",
    "    resp_history_df.to_csv(loc_history_path, index=False)\n",
    "    \n",
    "    return is_location\n",
    "\n",
    "\n",
    "def is_conceptnet_activity(concept, question):\n",
    "    concept = '_'.join(concept.split())\n",
    "\n",
    "    senses = get_senses(concept)\n",
    "    len_senses = len(senses)\n",
    "    \n",
    "    if len_senses == 1:\n",
    "        is_activity = 'v' in senses\n",
    "\n",
    "    else:\n",
    "        pos_tags = extract_pos_by_word(question, concept)\n",
    "        is_activity = 'VERB' in pos_tags\n",
    "\n",
    "        if len_senses < 1 and not is_activity:\n",
    "            if concept in activity_response_history:\n",
    "                return activity_response_history[concept]\n",
    "\n",
    "            res = requests.get(f\"https://api.conceptnet.io/query?node=/c/en/{concept}&rel=/r/IsA\")\n",
    "            obj = res.json()\n",
    "            \n",
    "            for edge in obj['edges']:\n",
    "                end_edge = edge['end']['@id'].split('/')\n",
    "                if 'activity' in end_edge or 'intelligent_agent_activity' in end_edge:\n",
    "                    is_activity = True\n",
    "                    break\n",
    "            \n",
    "            activity_response_history[concept] = is_activity\n",
    "            resp_history_df = pd.DataFrame({'prompt': activity_response_history.keys(), 'response': activity_response_history.values()})\n",
    "            resp_history_df.to_csv(activity_history_path, index=False)\n",
    "    \n",
    "    return is_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response history found!\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.organization = os.environ['OPENAI_UILAB_KEY']\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "resp_history_filename = f\"{out_parent_dir}{model_name}_history_{lang_name}_230823.csv\"\n",
    "resp_history_path = Path(resp_history_filename)\n",
    "if resp_history_path.is_file():\n",
    "    print(\"Response history found!\")\n",
    "    resp_history_df = pd.read_csv(resp_history_filename)\n",
    "    response_history = dict(zip(resp_history_df.prompt, resp_history_df.response))\n",
    "else:\n",
    "    print(\"Response history not found. Initializing new one...\")\n",
    "    response_history = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_prompt(concept, question, concept_type, location_name):\n",
    "    start_prompt = f\"Text: {question}\\nConcept: {concept}\\n\\n\"\n",
    "    end_prompt = \"Answer with 'yes' or 'no'.\"\n",
    "\n",
    "    if concept_type == \"activity\":\n",
    "        if has_multi_sense(concept):\n",
    "            return [\n",
    "                f\"{start_prompt}Can one '{concept}' in {location_name}? {end_prompt}\",\n",
    "                f\"{start_prompt}Do people in {location_name} familiar with '{concept}' concept? {end_prompt}\",\n",
    "                f\"{start_prompt}Is '{concept}' concept exist in {location_name}? {end_prompt}\",\n",
    "                f\"{start_prompt}In {location_name}, can people {concept}? {end_prompt}\",\n",
    "                f\"{start_prompt}Can '{concept}' be done in {location_name}? {end_prompt}\"\n",
    "            ]\n",
    "        return [\n",
    "            f\"Can one '{concept}' in {location_name}? {end_prompt}\",\n",
    "            f\"Do people in {location_name} familiar with '{concept}' concept? {end_prompt}\",\n",
    "            f\"Is '{concept}' concept exist in {location_name}? {end_prompt}\",\n",
    "            f\"In {location_name}, can people {concept}? {end_prompt}\",\n",
    "            f\"Can '{concept}' be done in {location_name}? {end_prompt}\"\n",
    "        ]\n",
    "    else:\n",
    "        if has_multi_sense(concept):\n",
    "            return [\n",
    "                f\"{start_prompt}Can one find '{concept}' in {location_name}? {end_prompt}\",\n",
    "                f\"{start_prompt}Do people in {location_name} familiar with '{concept}' concept? {end_prompt}\",\n",
    "                f\"{start_prompt}Is '{concept}' concept exist in {location_name}? {end_prompt}\",\n",
    "                f\"{start_prompt}In {location_name}, can people find {concept}? {end_prompt}\",\n",
    "                f\"{start_prompt}Can '{concept}' concept be found in {location_name}? {end_prompt}\"\n",
    "            ]\n",
    "        return [\n",
    "            f\"Can people find '{concept}' in {location_name}? {end_prompt}\",\n",
    "            f\"Do people in {location_name} familiar with '{concept}' concept? {end_prompt}\",\n",
    "            f\"Is '{concept}' concept exist in {location_name}? {end_prompt}\",\n",
    "            f\"In {location_name}, can people find {concept}? {end_prompt}\",\n",
    "            f\"Can '{concept}' concept be found in {location_name}? {end_prompt}\"\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_openai_chat_completion(input_prompt, model_name, resp_num):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': input_prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=1,\n",
    "        n=resp_num\n",
    "    )\n",
    "\n",
    "\n",
    "def get_openai_relevancy(input_prompt, model_name, resp_num=15):\n",
    "    if input_prompt in response_history:\n",
    "        return response_history[input_prompt]\n",
    "    \n",
    "    try:\n",
    "        completion = get_openai_chat_completion(input_prompt, model_name, resp_num)\n",
    "    except Exception:\n",
    "        time.sleep(60)\n",
    "        completion = get_openai_chat_completion(input_prompt, model_name, resp_num)\n",
    "        \n",
    "    responses = [resp.message.content.strip().lower() for resp in completion.choices]\n",
    "    assert len(responses) == resp_num\n",
    "\n",
    "    count_yes = responses.count('yes')\n",
    "    count_no = responses.count('no')\n",
    "    if count_yes == count_no:\n",
    "        response_final = 'neutral'\n",
    "    elif count_yes > count_no:\n",
    "        response_final = 'yes'\n",
    "    else:\n",
    "        response_final = 'no'\n",
    "\n",
    "    response_history[input_prompt] = response_final\n",
    "    resp_history_df = pd.DataFrame({'prompt': response_history.keys(), 'response': response_history.values()})\n",
    "    resp_history_df.to_csv(resp_history_filename, index=False)\n",
    "    \n",
    "    return response_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset commonsense_qa (/media/kiki/kiki_hdd/cache/commonsense_qa/default/1.0.0/28d68f56649a7f0c23bc68eae850af914aa03f95f810011ae8cf58cc5ff5051b)\n"
     ]
    }
   ],
   "source": [
    "split_name = 'train'\n",
    "en_csqa = load_dataset('commonsense_qa', split=split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9741/9741 [24:31<00:00,  6.62it/s]\n"
     ]
    }
   ],
   "source": [
    "relevancy_data = {\n",
    "    'q_id': [], 'question': [], 'q_concept': [],\n",
    "    'option_a': [], 'option_b': [], 'option_c': [], 'option_d': [], 'option_e': [],\n",
    "    'names': []\n",
    "}\n",
    "option_idxs = ['option_a', 'option_b', 'option_c', 'option_d', 'option_e']\n",
    "for item in tqdm(en_csqa):\n",
    "    # Check q concept offensiveness\n",
    "    options_str = ' '.join(item['choices']['text']).lower()\n",
    "    is_offensive = any(ex in item['question'].lower() for ex in excluded_concepts) or \\\n",
    "        any(ex in options_str for ex in excluded_concepts)\n",
    "    if is_offensive:\n",
    "        continue\n",
    "    \n",
    "    relevancy_data['q_id'].append(item['id'])\n",
    "    relevancy_data['question'].append(item['question'])\n",
    "\n",
    "    # Check concept generality\n",
    "    is_general_concepts = any(c in item['question_concept'] for c in general_concepts) or is_filler(item['question_concept'], item['question'])\n",
    "    if is_general_concepts:\n",
    "        relevancy_data['q_concept'].append(['yes'] * 5)\n",
    "    else:\n",
    "        concept_type = \"activity\" if is_conceptnet_activity(item['question_concept'], item['question']) else \"other\"\n",
    "        input_prompts = get_input_prompt(item['question_concept'], item['question'], concept_type, location_context)\n",
    "        assert len(input_prompts) == 5\n",
    "        relevancy_data['q_concept'].append([get_openai_relevancy(input_prompt, model_name) for input_prompt in input_prompts])\n",
    "\n",
    "    for option_idx, choice in zip(option_idxs, item['choices']['text']):\n",
    "        locations = extract_locations(choice)\n",
    "        if len(locations) > 0 or is_conceptnet_location(choice):\n",
    "            relevancy_data[option_idx].append(\"no\") # means irrelevant, has location\n",
    "        else:\n",
    "            relevancy_data[option_idx].append(\"yes\")\n",
    "        \n",
    "    names = extract_names(item['question'])\n",
    "    relevancy_data['names'].append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevancy_df = pd.DataFrame(relevancy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>question</th>\n",
       "      <th>q_concept</th>\n",
       "      <th>option_a</th>\n",
       "      <th>option_b</th>\n",
       "      <th>option_c</th>\n",
       "      <th>option_d</th>\n",
       "      <th>option_e</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>075e483d21c29a511267ef62bedc0461</td>\n",
       "      <td>The sanctions against the school were a punish...</td>\n",
       "      <td>[yes, no, no, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61fe6e879ff18686d7552425a36344c8</td>\n",
       "      <td>Sammy wanted to go to where the people were.  ...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Sammy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4c1cb0e95b99f72d55c068ba0255c54d</td>\n",
       "      <td>To locate a choker not located in a jewelry bo...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02e821a3e53cb320790950aab4489e85</td>\n",
       "      <td>Google Maps and other highway and street GPS s...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23505889b94e880c3e89cff4ba119860</td>\n",
       "      <td>The fox walked from the city into the forest, ...</td>\n",
       "      <td>[yes, no, yes, no, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9516</th>\n",
       "      <td>f1b2a30a1facff543e055231c5f90dd0</td>\n",
       "      <td>What would someone need to do if he or she wan...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9517</th>\n",
       "      <td>a63b4d0c0b34d6e5f5ce7b2c2c08b825</td>\n",
       "      <td>Where might you find a chair at an office?</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9518</th>\n",
       "      <td>22d0eea15e10be56024fd00bb0e4f72f</td>\n",
       "      <td>Where would you buy jeans in a place with a la...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9519</th>\n",
       "      <td>7c55160a4630de9690eb328b57a18dc2</td>\n",
       "      <td>John fell down the well.  he couldn't believe ...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>[John]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9520</th>\n",
       "      <td>dd640927f9920930501fb8dc3efc196b</td>\n",
       "      <td>I forgot to pay the electricity bill, now what...</td>\n",
       "      <td>[yes, yes, yes, yes, yes]</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9521 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  q_id  \\\n",
       "0     075e483d21c29a511267ef62bedc0461   \n",
       "1     61fe6e879ff18686d7552425a36344c8   \n",
       "2     4c1cb0e95b99f72d55c068ba0255c54d   \n",
       "3     02e821a3e53cb320790950aab4489e85   \n",
       "4     23505889b94e880c3e89cff4ba119860   \n",
       "...                                ...   \n",
       "9516  f1b2a30a1facff543e055231c5f90dd0   \n",
       "9517  a63b4d0c0b34d6e5f5ce7b2c2c08b825   \n",
       "9518  22d0eea15e10be56024fd00bb0e4f72f   \n",
       "9519  7c55160a4630de9690eb328b57a18dc2   \n",
       "9520  dd640927f9920930501fb8dc3efc196b   \n",
       "\n",
       "                                               question  \\\n",
       "0     The sanctions against the school were a punish...   \n",
       "1     Sammy wanted to go to where the people were.  ...   \n",
       "2     To locate a choker not located in a jewelry bo...   \n",
       "3     Google Maps and other highway and street GPS s...   \n",
       "4     The fox walked from the city into the forest, ...   \n",
       "...                                                 ...   \n",
       "9516  What would someone need to do if he or she wan...   \n",
       "9517         Where might you find a chair at an office?   \n",
       "9518  Where would you buy jeans in a place with a la...   \n",
       "9519  John fell down the well.  he couldn't believe ...   \n",
       "9520  I forgot to pay the electricity bill, now what...   \n",
       "\n",
       "                      q_concept option_a option_b option_c option_d option_e  \\\n",
       "0       [yes, no, no, yes, yes]      yes      yes      yes      yes      yes   \n",
       "1     [yes, yes, yes, yes, yes]      yes      yes      yes      yes      yes   \n",
       "2     [yes, yes, yes, yes, yes]      yes      yes      yes      yes      yes   \n",
       "3     [yes, yes, yes, yes, yes]       no       no      yes      yes      yes   \n",
       "4       [yes, no, yes, no, yes]      yes      yes      yes      yes      yes   \n",
       "...                         ...      ...      ...      ...      ...      ...   \n",
       "9516  [yes, yes, yes, yes, yes]      yes      yes      yes      yes      yes   \n",
       "9517  [yes, yes, yes, yes, yes]      yes      yes      yes      yes      yes   \n",
       "9518  [yes, yes, yes, yes, yes]      yes      yes      yes      yes      yes   \n",
       "9519  [yes, yes, yes, yes, yes]      yes      yes      yes       no      yes   \n",
       "9520  [yes, yes, yes, yes, yes]      yes      yes      yes      yes      yes   \n",
       "\n",
       "        names  \n",
       "0          []  \n",
       "1     [Sammy]  \n",
       "2          []  \n",
       "3          []  \n",
       "4          []  \n",
       "...       ...  \n",
       "9516       []  \n",
       "9517       []  \n",
       "9518       []  \n",
       "9519   [John]  \n",
       "9520       []  \n",
       "\n",
       "[9521 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevancy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parent_dir = \"../dataset/relevancy_context/\"\n",
    "relevancy_df.to_csv(out_parent_dir + split_name + \"_\" + lang_name + \"_relevancy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
